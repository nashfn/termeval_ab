# =============================================================================
# LLM API Keys
# =============================================================================
# Set the API key for your chosen provider

# Anthropic
ANTHROPIC_API_KEY=

# OpenAI
OPENAI_API_KEY=

# Google
GOOGLE_API_KEY=

# Azure OpenAI
AZURE_API_KEY=
AZURE_API_VERSION=2024-02-15-preview

# AWS Bedrock
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION_NAME=us-east-1

# Together AI
TOGETHER_API_KEY=

# Groq
GROQ_API_KEY=

# =============================================================================
# Custom LLM Endpoints (OpenAI-compatible)
# =============================================================================
# For custom/self-hosted endpoints that use OpenAI-compatible API

# Nebius Cloud
NEBIUS_API_KEY=
NEBIUS_API_BASE=https://api.studio.nebius.ai/v1

# Generic OpenAI-compatible endpoint
OPENAI_API_BASE=
# Example: OPENAI_API_BASE=https://your-custom-endpoint.com/v1

# Azure OpenAI endpoint
AZURE_API_BASE=
# Example: AZURE_API_BASE=https://your-resource.openai.azure.com

# Ollama (local)
OLLAMA_API_BASE=http://localhost:11434

# =============================================================================
# LiteLLM Configuration
# =============================================================================

# Enable verbose logging for debugging
LITELLM_VERBOSE=false

# Set default timeout (seconds)
LITELLM_REQUEST_TIMEOUT=120

# Retry configuration
LITELLM_NUM_RETRIES=3

# =============================================================================
# Model Configuration
# =============================================================================
# Set default model via environment variable (optional)
# The --model CLI flag takes precedence

# LLM_MODEL=anthropic/claude-sonnet-4-20250514
# LLM_MODEL=openai/gpt-4o
# LLM_MODEL=nebius/meta-llama/Meta-Llama-3.1-70B-Instruct
# LLM_MODEL=ollama/llama3

# =============================================================================
# Provider-Specific Configuration
# =============================================================================
#
# NEBIUS CLOUD:
#   Model format: nebius/<model-name>
#   Examples:
#     nebius/meta-llama/Meta-Llama-3.1-70B-Instruct
#     nebius/meta-llama/Meta-Llama-3.1-8B-Instruct
#     nebius/Qwen/Qwen2.5-72B-Instruct
#   Requires: NEBIUS_API_KEY, NEBIUS_API_BASE
#
#   Usage:
#     python purple-agent/src/server.py --model nebius/meta-llama/Meta-Llama-3.1-70B-Instruct
#
# OPENAI-COMPATIBLE (vLLM, TGI, text-generation-webui, etc.):
#   Model format: openai/<model-name>
#   Set OPENAI_API_BASE to your endpoint
#   Example:
#     OPENAI_API_BASE=http://localhost:8000/v1
#     python purple-agent/src/server.py --model openai/mistral-7b
#
# OLLAMA (local):
#   Model format: ollama/<model-name>
#   Examples: ollama/llama3, ollama/codellama, ollama/mistral
#   Default endpoint: http://localhost:11434
#   Override: OLLAMA_API_BASE=http://your-ollama-server:11434
#
# AZURE OPENAI:
#   Model format: azure/<deployment-name>
#   Requires: AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION
#   Example:
#     AZURE_API_BASE=https://my-resource.openai.azure.com
#     python purple-agent/src/server.py --model azure/gpt-4-deployment
#
# AWS BEDROCK:
#   Model format: bedrock/<model-id>
#   Example: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
#   Requires: AWS credentials configured (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
#
# TOGETHER AI:
#   Model format: together_ai/<model-name>
#   Example: together_ai/meta-llama/Llama-3-70b-chat-hf
#   Requires: TOGETHER_API_KEY
#
# GROQ:
#   Model format: groq/<model-name>
#   Example: groq/llama3-70b-8192
#   Requires: GROQ_API_KEY
